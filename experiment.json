{
  "description.sk": [
    "GAN stability \n",
    "} \nYou can find further details on our project page. \nUsage \nFirst download your data and put it into the ./data folder. \nTo train a new model, first create a config script similar to the ones provided in the ./configs folder.  You can then train you model using \npython train.py PATH_TO_CONFIG \n",
    "python test.py PATH_TO_CONFIG \n",
    "python interpolate_class.py PATH_TO_CONFIG \nPretrained models \nWe also provide several pretrained models. \n",
    "python test.py PATH_TO_CONFIG \n",
    "configs/pretrained/celebA_pretrained.yaml \nconfigs/pretrained/celebAHQ_pretrained.yaml \nconfigs/pretrained/imagenet_pretrained.yaml \nconfigs/pretrained/lsun_bedroom_pretrained.yaml \nconfigs/pretrained/lsun_bridge_pretrained.yaml \nconfigs/pretrained/lsun_church_pretrained.yaml \nconfigs/pretrained/lsun_tower_pretrained.yaml \nOur script will automatically download the model checkpoints and run the generation. \nYou can find the outputs in the output/pretrained folders. \n",
    "Notes \n",
    "Code for paper \"Which Training Methods for GANs do actually Converge? (ICML 2018)\""
  ],
  "invocation.sk": [
    "@INPROCEEDINGS{Mescheder2018ICML, \n  author = {Lars Mescheder and Sebastian Nowozin and Andreas Geiger}, \n  title = {Which Training Methods for GANs do actually Converge?}, \n  booktitle = {International Conference on Machine Learning (ICML)}, \n  year = {2018} \n",
    "To compute the inception score for your model and generate samples, use \n",
    "Finally, you can create nice latent space interpolations using \n",
    "or \n",
    "You can use the models for sampling by entering \n",
    "where PATH_TO_CONFIG is one of the config files \n",
    "Similarly, you can use the scripts interpolate.py and interpolate_class.py for generating interpolations for the pretrained models. \nPlease note that the config files  *_pretrained.yaml are only for generation, not for training new models: when these configs are used for training, the model will be trained from scratch, but during inference our code will still use the pretrained model. \n",
    "Batch normalization is currently not supported when using an exponential running average, as the running average is only computed over the parameters of the models and not the other buffers of the model. \n"
  ],
  "installation.sk": [
    "python interpolate.py PATH_TO_CONFIG \n"
  ],
  "citation.sk": [
    "This repository contains the experiments in the supplementary material for the paper Which Training Methods for GANs do actually Converge?. \nTo cite this work, please use \n"
  ],
  "name": "GAN_stability",
  "owner": "LMescheder",
  "license": {
    "name": "MIT License",
    "url": "https://api.github.com/licenses/mit"
  },
  "forks_url": "https://api.github.com/repos/LMescheder/GAN_stability/forks",
  "topics": [],
  "languages": [
    "Jupyter Notebook",
    "Python",
    "Shell"
  ],
  "readme_url": "https://github.com/LMescheder/GAN_stability/blob/master/README.md",
  "releases": []
}